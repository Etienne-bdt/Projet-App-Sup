{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TVF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from network import Modele\n",
    "from tqdm import tqdm\n",
    "from utils import class_weights\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Dataset class for our change detection dataset\n",
    "\n",
    "class ChangeDetectionDataset(Dataset):\n",
    "    def __init__(self, csv_file=\"data.csv\", data_dir=\"./data\", batch_size=1, transform=None, crop_size=128):\n",
    "        #repeat the data 5 times to have more data\n",
    "        self.data = pd.read_csv(csv_file).sample(frac=20, replace=True)\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.batch_size = batch_size\n",
    "        self.crop_size = crop_size\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def random_crop(self, img1, img2, cm, size):\n",
    "        x = np.random.randint(0, img1.shape[2]-size)\n",
    "        y = np.random.randint(0, img1.shape[1]-size)\n",
    "        img1 = img1[:,y:y+size, x:x+size]\n",
    "        img2 = img2[:,y:y+size, x:x+size]\n",
    "        cm = cm[0:1, y:y+size, x:x+size]\n",
    "        return img1, img2, cm\n",
    "\n",
    "    def random_flip(self, img1,img2,cm, chance=0.5):\n",
    "        if (np.random.randint(0,1)> chance):\n",
    "            img1 = TVF.hflip(img1)\n",
    "            img2 = TVF.hflip(img2)\n",
    "            cm = TVF.hflip(cm)\n",
    "\n",
    "        if (np.random.randint(0,1)> chance):\n",
    "            img1 = TVF.vflip(img1)\n",
    "            img2 = TVF.vflip(img2)\n",
    "            cm = TVF.vflip(cm)\n",
    "\n",
    "        return img1, img2, cm\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img1 = read_image(self.data_dir+'/'+self.data.iloc[idx,0])\n",
    "        img2 = read_image(self.data_dir+'/'+self.data.iloc[idx,1])\n",
    "        cm = read_image(self.data_dir+'/'+self.data.iloc[idx,2])\n",
    "\n",
    "        img1Tensor = torch.zeros((3, self.crop_size, self.crop_size), dtype=torch.float32)\n",
    "        img2Tensor = torch.zeros((3, self.crop_size, self.crop_size), dtype=torch.float32)\n",
    "        cmTensor = torch.zeros((1, self.crop_size, self.crop_size), dtype=torch.float32)\n",
    "        \n",
    "        crop1, crop2, cropcm = self.random_crop(img1[:,:,:], img2[:,:,:], cm[:,:,:], self.crop_size)\n",
    "        crop1, crop2, cropcm = self.random_flip(crop1[:,:,:], crop2[:,:,:], cropcm[:,:,:])\n",
    "        img1Tensor[:,:,:] = crop1.float()/255\n",
    "        img2Tensor[:,:,:] = crop2.float()/255\n",
    "        cmTensor[:,:,:] = cropcm.float()/255\n",
    "        #apply the same transformation to all images as batch dimension\n",
    "        return img1Tensor, img2Tensor, cmTensor\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arceo\\AppData\\Local\\Temp\\ipykernel_36212\\438349489.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  weights = torch.tensor(weights[1]/weights[0]).to(device)\n"
     ]
    }
   ],
   "source": [
    "#Simple DataLoader class for our change detection dataset\n",
    "batch_size = 16\n",
    "\n",
    "weights = class_weights(\"data.csv\")\n",
    "weights = torch.tensor(weights[1]/weights[0]).to(device)\n",
    "\n",
    "train_dataset= ChangeDetectionDataset(data_dir=\"data\",csv_file=\"train.csv\", batch_size=batch_size, transform=None)\n",
    "val_dataset = ChangeDetectionDataset(data_dir=\"data\",csv_file=\"val.csv\", batch_size=1, transform=None)\n",
    "train_loader = DataLoader(batch_size=batch_size, dataset=train_dataset, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=True)\n",
    "#Simple function to display a batch of images\n",
    "\n",
    "def show_batch(batch):\n",
    "    img1s, img2s, cms = batch\n",
    "\n",
    "    for i in range(len(img1s)):\n",
    "        img1 = img1s[i,:,:,:]\n",
    "        img2 = img2s[i,:,:,:]\n",
    "        cm = cms[i,:,:,:]\n",
    "        fig, ax = plt.subplots(1,3)\n",
    "        ax[0].imshow(img1.permute(1,2,0))\n",
    "        ax[1].imshow(img2.permute(1,2,0))\n",
    "        ax[2].imshow(cm.permute(1,2,0), cmap='gray')\n",
    "        plt.show()\n",
    "\n",
    "#a = next(iter(train_loader))\n",
    "#show_batch(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|==========| 24/24 [00:14<00:00,  1.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : Training, loss: 0.7341694235801697, accuracy: 0.8535774946212769 | Validation, loss: 0.7057780027389526, accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|==========| 24/24 [00:13<00:00,  1.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 : Training, loss: 0.7046517729759216, accuracy: 0.9461609125137329 | Validation, loss: 0.6965171098709106, accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|==========| 24/24 [00:14<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 : Training, loss: 0.6858428716659546, accuracy: 0.5260029435157776 | Validation, loss: 0.6824877858161926, accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|==========| 24/24 [00:13<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 : Training, loss: 0.6796907782554626, accuracy: 0.08978279680013657 | Validation, loss: 0.679288387298584, accuracy: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|==>       | 7/24 [00:04<00:10,  1.65it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m loss_cumu \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     15\u001b[0m prec \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 16\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m >=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimg2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#Forward pass\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[0;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[0;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 38\u001b[0m, in \u001b[0;36mChangeDetectionDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     36\u001b[0m img1 \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx,\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     37\u001b[0m img2 \u001b[38;5;241m=\u001b[39m read_image(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39miloc[idx,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 38\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43mread_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m img1Tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_size), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     41\u001b[0m img2Tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcrop_size), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torchvision\\io\\image.py:297\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path, mode, apply_exif_orientation)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    296\u001b[0m     _log_api_usage_once(read_image)\n\u001b[1;32m--> 297\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decode_image(data, mode, apply_exif_orientation\u001b[38;5;241m=\u001b[39mapply_exif_orientation)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torchvision\\io\\image.py:53\u001b[0m, in \u001b[0;36mread_file\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m     52\u001b[0m     _log_api_usage_once(read_file)\n\u001b[1;32m---> 53\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\_ops.py:1061\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[0;32m   1060\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(self_, args, kwargs)\n\u001b[1;32m-> 1061\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mself_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epoch = 50\n",
    "learning_rate = 0.0005\n",
    "model = Modele()\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_loss, val_loss = [], []\n",
    "train_precision, val_precision = [], []\n",
    "model = model.to(device)\n",
    "best_loss =1000\n",
    "\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    model.train()\n",
    "    loss_cumu = 0\n",
    "    prec = 0\n",
    "    for img1,img2,cm in tqdm(train_loader, ascii=\" >=\"):\n",
    "        img1,img2,cm = img1.to(device),img2.to(device),cm.to(device)\n",
    "        #Forward pass\n",
    "        y_pred = model(img1, img2)\n",
    "        loss = loss_fn(y_pred, cm)\n",
    "        loss_cumu += loss\n",
    "        #Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prec += torch.sum((torch.round(torch.sigmoid(y_pred))*cm))/(torch.sum(cm)+1)\n",
    "    train_loss.append(loss_cumu/len(train_loader))\n",
    "    train_precision.append(prec/len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    loss_cumu=0\n",
    "    prec=0\n",
    "    with torch.no_grad():\n",
    "        for img1,img2,cm in val_loader:\n",
    "            img1,img2,cm = img1.to(device),img2.to(device),cm.to(device)\n",
    "            y_pred=model(img1,img2)\n",
    "            #y_pred, cm = torch.flatten(y_pred, start_dim=1), torch.flatten(cm, start_dim=1)\n",
    "            loss = loss_fn(y_pred,cm)\n",
    "            loss_cumu += loss\n",
    "            prec += torch.sum((torch.round(torch.sigmoid(y_pred))*cm))/(torch.sum(cm)+1)\n",
    "            if(loss_cumu/len(val_loader) < best_loss):\n",
    "                best_loss = loss_cumu/len(val_loader)\n",
    "                torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        val_loss.append(loss_cumu/len(val_loader))\n",
    "        val_precision.append(prec/len(val_loader))\n",
    "    print(f\"Epoch {epoch+1} : Training, loss: {train_loss[-1]}, accuracy: {train_precision[-1]} | Validation, loss: {val_loss[-1]}, accuracy: {val_precision[-1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model.eval()\n",
    "im1,im2,cm = next(iter(train_loader))\n",
    "cm_pred = model(im1,im2, with_attn=False)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(cm[0].permute(1,2,0), cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(cm_pred.detach()[0].permute(1,2,0), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(im1[0].permute(1,2,0))\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(im2[0].permute(1,2,0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"best_model.pt\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "U-Net peut être utile\n",
    "\n",
    "# Soutenance\n",
    " - Explication du problème et comment le transcrire\n",
    " - Pré-traitement des données\n",
    " - Architecture du réseau\n",
    " - Présentation des résultats\n",
    " \n",
    "# Rendu \n",
    " - Slides de présentation (10 minutes+ 10 min de questions)\n",
    " - Notebook avec le code\n",
    "\n",
    "À rendre en séance. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
